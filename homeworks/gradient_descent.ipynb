{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "plt.rc('font',family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=1000 # d: dimension\n",
    "n=2000 # n: number of points\n",
    "A = np.random.normal(size=(n,d)) / np.sqrt(n) # matrix containing the data points\n",
    "y = np.random.normal(size=n)\n",
    "lambd= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the Ridge cost function:\n",
    "$$\n",
    "f(x) = \\frac{1}{2} \\|Ax-y\\|^2 + \\frac{\\lambda}{2} \\|x\\|^2,\n",
    "$$\n",
    "where $\\lambda > 0$ is some regularization parameter that we take equal to $1$. The matrix $A$ and the vector $y$ are defined in the cell above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Show that $f$ is can be written in the format the function $f$ of Problem 12.2, for some $M \\in \\mathbb{R}^{d \\times d}$, $b \\in \\mathbb{R}^d$ and $c \\in \\mathbb{R}$. Compute numerically the values of $L$ and $\\mu$. Plot the eigenvalues of $H_f(x)$ using an histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Implement gradient descent with constant step-size $\\beta = 1/L$ (as in Problem 12.2), with random initial position $x_0$. Plot the log-error $\\log (\\|x_t - x_*\\|)$ as a function of $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Implement gradient descent with momentum, with the same parameters as in Problem 12.4. Plot the log-error $\\log (\\|x_t - x_*\\|)$ as a function of $t$, on the same plot than the log-error of gradient descent without momentum.\n",
    "On the same plot, plot also the lines of equation\n",
    "$$\n",
    "y = \\log(1-\\mu / L) \\times t \\qquad \\text{and} \\qquad\n",
    "y = \\log\\Big(\\frac{\\sqrt{L}-\\sqrt{\\mu}}{\\sqrt{L}+\\sqrt{\\mu}}\\Big) \\times t.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
