{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression as linear least squares\n",
    "\n",
    "From the knowledge of a sample of pair of scalar values $\\{a_i, y_i\\}_{i=1}^n$, we would like to predict the relation between $x$ and $y$. One simple way to go beyond linear regression is to consider polynomial regression: for example we could try to model $y$ as a polynomial of degree 3 of $a$. We would look for $(x_0, x_1, x_2, x_3) \\in \\mathbb{R}^4$ such that the values $a_i$ and $y_i$ are linked as $y_i \\simeq x_0 + x_1 a_i + x_2 a_i^2$.\n",
    "\n",
    "This problem can be mapped to linear regression by considering that we have for each $a$ a feature vectors of dimension $d + 1$ when considering the fit of a polynomial of degree $d$. This feature vector is $(1, a, a^2, \\cdots, a^d)$. Such that the full data matrix is\n",
    "$$\n",
    "    A = \\left[\\begin{matrix}\n",
    "    1 & a_1 & \\cdots & a_1^d \\\\\n",
    "    1 & a_2 & \\cdots & a_2^d \\\\ \n",
    "    \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "    1 & a_n & \\cdots & a_n^d\n",
    "\\end{matrix} \\right] \\in \\mathbb{R}^{n \\times (d+1)}.\n",
    "$$\n",
    "\n",
    "As a exercise below we will consider data that was created from a polynomial of dimension 3, to which noise is added. Assuming that we do not know the degree of the generated polynomial, we will try to fit with $d = 5$ and $d=2$ and investigate ridge regression.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions to setup the problem\n",
    "def get_data_mat(a, deg):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    a: (np.array of size N)\n",
    "    deg: (int) max degree used to generate the data matrix\n",
    "    \n",
    "    Returns:\n",
    "    A: (np.array of size N x (deg_true + 1)) data matrix\n",
    "    \"\"\"\n",
    "    A = np.array([a ** i for i in range(deg + 1)]).T\n",
    "    return A\n",
    "\n",
    "def draw_sample(deg_true, x, N, eps=0):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    deg_true: (int) degree of the polynomial g\n",
    "    a: (np.array of size deg_true) parameter of g\n",
    "    N: (int) size of sample to draw\n",
    "    eps: noise level\n",
    "    \n",
    "    Returns:\n",
    "    x: (np.array of size N)\n",
    "    y: (np.array of size N)\n",
    "    \"\"\"    \n",
    "    a = np.sort(np.random.rand(N))\n",
    "    A = get_data_mat(a, deg_true)\n",
    "    y = A @ x + eps * np.random.randn(N)\n",
    "    return a, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** Complete the three functions below to obtain \n",
    "- the least square estimator $x^{LS}$\n",
    "- the ridge estimator $x^{Ridge}$\n",
    "- the mean square error $\\lVert A x - y \\rVert^2 / n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square_estimator(A, y):\n",
    "    #Your code\n",
    "\n",
    "\n",
    "def ridge_estimator(A, y, lbd):\n",
    "    #Your code\n",
    "\n",
    "\n",
    "def mean_squared_error(x, A, y):\n",
    "    #Your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cells generates the data - for your submission do not change it. \n",
    "# But for your own curiosity, do not hesistate to investigate what is going on when changing the properties of the data :) \n",
    "\n",
    "np.random.seed(45) # fixing seed so everyone should see the same data\n",
    "N = 10\n",
    "deg_true = 3 # degree of true polynomial\n",
    "eps = 0.5 # noise amplitude\n",
    "x_true = np.array([-3.75307359,  6.58178662,  6.23070014, -8.02457871])\n",
    "\n",
    "# radom input data\n",
    "a_tr, y_tr = draw_sample(deg_true, x_true, N, eps=eps) # training data\n",
    "a_te, y_te = draw_sample(deg_true, x_true, 2 * N, eps=eps) # testing data\n",
    "\n",
    "a_plot = np.linspace(0, 1, 100)\n",
    "A_plot = get_data_mat(a_plot, deg_true)\n",
    "plt.plot(a_tr, y_tr,'o', label='train')\n",
    "plt.plot(a_te, y_te,'o', label='test')\n",
    "plt.plot(a_plot, A_plot @ x_true,'-', label='true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** Complete the code below to visualize the prediction of $x^{LS}$ and $x^{Ridge}$ for $\\lambda$ in $[1e-7, 0.1, 1]$, using in all cases a prediction model of degree 5. The output of the cell should be a plot as above, where you added three lines of predictions for all values of $a \\in [0,1]$: line LS, line ridge $\\lambda = 1e-7$, line ridge $\\lambda = 0.1$, line ridge $\\lambda = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_plot = np.linspace(0, 1, 100)\n",
    "A_plot = get_data_mat(a_plot, deg_true)\n",
    "plt.plot(a_tr, y_tr,'o', label='train')\n",
    "plt.plot(a_te, y_te,'o', label='test')\n",
    "plt.plot(a_plot, A_plot @ x_true,'-', label='true')\n",
    "\n",
    "deg_pred = 5\n",
    "\n",
    "A_tr = get_data_mat(a_tr, deg_pred)\n",
    "A_te = get_data_mat(a_te, deg_pred)\n",
    "\n",
    "x_ls = #your code ..\n",
    "\n",
    "A_plot = get_data_mat(a_plot, deg_pred)\n",
    "plt.plot(a_plot, A_plot @ x_ls, label='LS')\n",
    "\n",
    "for lbd in [1e-7, 0.1, 1]:\n",
    "   #your code\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** Use the `mean_squared_error` to make a plot of the training error and the test error as a function of $\\lambda$ as we have seen in the lecture (range given below). Which value of $\\lambda$ would you choose? Does that align with your intuition from the plots above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_mse = []\n",
    "te_mse = []\n",
    "lbds = np.logspace(-6, -1, 10)\n",
    "for lbd in lbds:\n",
    "    # your code\n",
    "    tr_mse.append(# your code)\n",
    "    te_mse.append(# your code)\n",
    "\n",
    "plt.plot(lbds, tr_mse, '-+', label='train')\n",
    "plt.plot(lbds, te_mse,'-+', label='test')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** For the optimal value of $\\lambda$ compare $x^{LS}$, $x^{Ridge}$ and $x^{true}$.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Repeat the same operation with a fitting model of degree 2 (`deg_pred=2`). What are your findings related to the optimal degree of regularizations in this case? "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b6039351453e7badce42e118241bd5a1f566162df5101cf0d47719eef69eddf3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
